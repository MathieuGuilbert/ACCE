from sklearn.cluster import AgglomerativeClustering
from sklearn import metrics
import subprocess
import sys
import random
from math import sqrt
from basePartition import *
from CAM import *
from constraints import verify_constraints

#Write a given list in a file easely readable by R
#data= list of list
#path= path+name of the output file
def writeDataForR(data,path):
    f = open(path, "w")
    for l in data:
        line=""
        for e in l:
            line=line+str(e)+" "
        line=line+"\n"
        f.write(line)
    f.close()
    return

#cons= a constraint set
#MLpath and CLpath are the path to where the constraints are to be written
#we apply +1 because arrays begin at 1 in R
def writeConstraintsForR(MLpath,CLpath,cons):
    #ML
    f = open(MLpath, "w")
    #CL
    g = open(CLpath, "w")
    for (a,b,t) in cons:
        if(t==1):
            f.write(str(a+1)+" "+str(b+1)+ "\n")
        else: #t=-1
            g.write(str(a+1)+" "+str(b+1)+ "\n")
    g.close()
    f.close()
    return

#read the output file of an MPCKmeans execution
#return the partition generated by MPCKmeans as a list
#partfile= path to the file containing the partition
def readResR(partfile):
    p = [] #all base partitions
    debut=True
    with open(partfile, 'r') as f:
        for line in f:
            line = line.strip()
            if (line != '') and not debut:
                atr=''
                save=False
                for i in range(len(line)):
                    if line[i]==" ":
                        save=True
                    elif save and line[i]!="" :
                        atr=atr+line[i]
                p.append(int(atr))
            if(debut):
                debut=False
    #print(p)
    return p

def testMCPKM():
    subprocess.run(["Rscript","/home/mathieu/Documents/Doctorat/Stage/git/stage-involvd-mathieu-guilbert-2021/Devs/protocole/protocoleR.R",
    "/home/mathieu/Documents/Doctorat/Stage/git/stage-involvd-mathieu-guilbert-2021/Devs/protocole/Architecture/halfmoon/halfmoon.txt",
    "/home/mathieu/Documents/Doctorat/Stage/git/stage-involvd-mathieu-guilbert-2021/Devs/protocole/Architecture/halfmoon/constraints/halfmoonML0.txt",
    "/home/mathieu/Documents/Doctorat/Stage/git/stage-involvd-mathieu-guilbert-2021/Devs/protocole/Architecture/halfmoon/constraints/halfmoonCL0.txt",
    "2","/home/mathieu/Documents/Doctorat/Stage/git/stage-involvd-mathieu-guilbert-2021/Devs/protocole/Architecture/halfmoon/constraints/outputFirstMPCK.txt"],
    cwd="/home/mathieu/Documents")
    #print("end subprocess")
    res=readResR("/home/mathieu/Documents/Doctorat/Stage/git/stage-involvd-mathieu-guilbert-2021/Devs/protocole/Architecture/halfmoon/constraints/outputFirstMPCK.txt")
    #res=readResR("/home/mathieu/Documents/testWriteRes.txt")
    resARI=metrics.adjusted_rand_score(GroundTruth[0],res)
    #print(resARI)

#scriptName=Name of the target R script
#scriptPath=path to the R script
#dataFilePath= path to a file containing the data (in format easely readable by R)
#MLfile and CLfile: path to files containing the constraints (resp MLs and CLs)
#outputFile: file in which the result or the MCPKM execution will be written
#labels= GroundTruth labels for the given dataset
def lauchMCPKM(scriptName,scriptPath,dataFile,MLfile,CLfile,k,outputFile,labels):
    subprocess.run(["Rscript",scriptPath,dataFile,MLfile,CLfile,str(k),outputFile],
    cwd="/home/mathieu/Documents")
    #print("end subprocess")
    res=readResR(outputFile)
    return res

#n: number of elements in the database
#k_final: number of clusters in the consensus partition
#nRepeat: number times to repat the main loop
#return a matrix ith the resulting ARIs
def baselineMCPKM(currentDataPath,datasetName,data,n,labels,k_final,nRepeat,constrSets):
    print("-- MPCKM Baseline --")
    from pythonScript import Matrices , minimalSplit #put here to avoid circular imports
    #calcul du k maximum (2 min, 50 max)
    lim=int(sqrt(n))
    if(lim>50):
        lim=50
    if(lim<2):
        lim=2

    scriptName="Rscript"
    scriptPath="/home/mathieu/Documents/Doctorat/Stage/git/stage-involvd-mathieu-guilbert-2021/Devs/protocole/protocoleR.R"
    dataFile=currentDataPath+"/"+datasetName+".txt" #dataFile="/home/mathieu/Documents/Doctorat/Stage/git/stage-involvd-mathieu-guilbert-2021/Devs/protocole/Architecture/halfmoon/halfmoon.txt"

    resMatrix=[[0 for x in range(len(constrSets))] for y in range(nRepeat)]
    #for each constrSets
    for c in range(0,len(constrSets)):
        constraints=constrSets[c]
        MLfile=currentDataPath+"/constraints/"+datasetName+"ML"+str(c)+".txt" #MLfile="/home/mathieu/Documents/Doctorat/Stage/git/stage-involvd-mathieu-guilbert-2021/Devs/protocole/Architecture/halfmoon/constraints/halfmoonML0.txt"
        CLfile=currentDataPath+"/constraints/"+datasetName+"CL"+str(c)+".txt" #CLfile="/home/mathieu/Documents/Doctorat/Stage/git/stage-involvd-mathieu-guilbert-2021/Devs/protocole/Architecture/halfmoon/constraints/halfmoonCL0.txt"
        writeConstraintsForR(MLfile,CLfile,constraints)

        generateDirectory(currentDataPath+"/MPCKM/constrSet"+str(c))

        results=[]; resultsARI=[]; allBasePart=[]; allBasePartARI=[]; allMinSplit=[]; allDistMat=[]; allCAM=[]; allBasePartK=[]
        allPerBP=[]; allPerCP=[]
        #Repeat nRepeat times
        for i in range(0,nRepeat):
            generateDirectory(currentDataPath+"/MPCKM/constrSet"+str(c)+"/CP"+str(i))
            generateDirectory(currentDataPath+"/MPCKM/constrSet"+str(c)+"/CP"+str(i)+"/basePartitions")
            basePartitions=[]
            ARIs=[] #ARIs of the base partitions
            allK=[]
            allPer=[]
            #Generate 50 base partitions with MPCKM
            for j in range(0,50):
                k=random.randrange(2,lim+1,1)
                allK.append(k)
                outputFile=currentDataPath+"/MPCKM/constrSet"+str(c)+"/CP"+str(i)+"/basePartitions/BP"+str(j)+"_k="+str(k)+".txt"
                mkm=lauchMCPKM(scriptName,scriptPath,dataFile,MLfile,CLfile,k,outputFile,labels)
                basePartitions.append(mkm)
                #Compute the ARI of the base partition
                kARI=metrics.adjusted_rand_score(labels,mkm)
                ARIs.append(kARI)
                #verify constraints
                allPer.append(verify_constraints(mkm,constraints))
            writeBasePartition(currentDataPath+"/MPCKM/constrSet"+str(c)+"/CP"+str(i)+"/"+datasetName+"_BP_MPCKM_k=[2,"+str(lim)+"]_CS"+str(c)+"_CP"+str(i)+".txt",basePartitions)
            allBasePart.append(basePartitions)
            allBasePartARI.append(ARIs)
            allBasePartK.append(allK)
            allPerBP.append(allPer)

            #construction of the co-association matrix (CAM) and the distance matrix
            #All numbers are between 0 and 1 included
            CAM,distMat=Matrices(basePartitions,len(basePartitions),n)
            allDistMat.append(distMat)
            allCAM.append(CAM)

            #Consensus function using Single Link
            mat = np.array(distMat)
            #pour distance matrix, rajouter option affinity='precomputed'
            #clustering = AgglomerativeClustering(affinity='precomputed',linkage='average',n_clusters=k_final).fit(mat)
            clustering = AgglomerativeClustering(affinity='precomputed',linkage='single',n_clusters=k_final).fit(mat)
            clustering
            finalLabels=clustering.labels_
            results.append(finalLabels)
            #compute minimal split in order to compare with MiniZinc/Gecode
            minSplit=minimalSplit(distMat,finalLabels)
            allMinSplit.append(minSplit)
            #Computation of ARI
            ARI = metrics.adjusted_rand_score(labels,finalLabels)
            resultsARI.append(ARI)
            #Computation of the percentage of verified constraints
            per=verify_constraints(finalLabels,constraints)
            allPerCP.append(per)
            #add the results
            resMatrix[i][c]=round(ARI,3)

        #End
        currentPath=currentDataPath+"/MPCKM/constrSet"+str(c)
        writeConsensusPartitionAnalysis_C(currentPath+"/consensusPartitionAnalysis.txt",allMinSplit,resultsARI,allPerCP)
        for r in range(0,len(allMinSplit)):
            tmpPath=currentPath+"/CP"+str(r)
            writeConsensusPartition_C(tmpPath+"/ConsensusPartition"+str(r)+".txt",results[r],allMinSplit[r],resultsARI[r],allPerCP[r])
            writeMatrix(tmpPath+"/DistanceMatrix"+str(r)+".txt",allDistMat[r])
            writeMatrix(tmpPath+"/CAM"+str(r)+".txt",allCAM[r])

        writeBasePartitionAnalysis_C(currentPath+"/CP"+str(i)+"/basePartitionAnalysis"+str(i)+".txt",allBasePartARI,allBasePartK,allPerBP)
        #print("constrSet "+str(c)+" done")
        #return allCAM,allDistMat

    print("-- End MPCKM Baseline --")
    return resMatrix

def writeConsensusPartition_C(fileName,partition,split,ARI,per):
    f = open(fileName, "w")
    f.write("ARI: "+str(ARI)+"\n")
    f.write("minSplit: "+str(split)+"\n")
    f.write("verified constraints percentage: "+str(per)+"\n")
    f.write("Partition: \n")
    f.write(str(partition))
    f.close()
    return

#write the stats of all the consensus partition sets
def writeConsensusPartitionAnalysis_C(fileName,allMinSplit,consensusARI,allPerCP):
    f = open(fileName, "w")
    f.write("Analysis of all the consensus partition"+"\n\n")
    #ARI
    f.write("Consensus partition ARI analysis: Mean="+str(round(np.mean(consensusARI),3))+" ,Median="+str(round(np.median(consensusARI),3))+
          " ,Min="+str(round(min(consensusARI),3))+" ,Max="+str(round(max(consensusARI),3))+
          " ,Standard Deviation="+str(round(np.std(consensusARI),3))+ "\n")
    #MinSplit
    f.write("Consensus partition MinSplit analysis: Mean="+str(round(np.mean(allMinSplit),3))+" ,Median="+str(round(np.median(allMinSplit),3))+
        " ,Min="+str(round(min(allMinSplit),3))+" ,Max="+str(round(max(allMinSplit),3))+
        " ,Standard Deviation="+str(round(np.std(allMinSplit),3))+ "\n")
    #Percentage
    f.write("Consensus partition verified constraints analysis: Mean="+str(round(np.mean(allPerCP),3))+" ,Median="+str(round(np.median(allPerCP),3))+
        ",Min="+str(round(min(allPerCP),3))+" ,Max="+str(round(max(allPerCP),3))+
        " ,Standard Deviation="+str(round(np.std(allPerCP),3))+ "\n")
    f.close()
    return
